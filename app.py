# !pip install flask flask-cors
# !pip install scikit-learn
# !pip install openai==0.28.0

# !pip install google-generativeai
# !pip install tiktoken
# !pip install python-docx
# !pip install requests
# !pip install PyPDF2
# !pip install pandas
# !pip install beautifulsoup4
# !pip install pyngrok
# !pip install sentence-transformers
import os
import re
from flask import Flask, request, jsonify
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import openai
import google.generativeai as genai
from flask_cors import CORS
import tiktoken
from datetime import datetime
from docx import Document
import requests
import json
import PyPDF2
import pandas as pd
from bs4 import BeautifulSoup
import csv


import torch
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, util





from pyngrok import ngrok

# # Ø¶Ø¹ Ø§Ù„ØªÙˆÙƒÙ† Ù‡Ù†Ø§ Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØµ
# ngrok.set_auth_token("2p2aHTSheJ5BDmJFhMLBM6xDSnW_3KRzKLCTRRqEEaF2ettNr")





# Ø¥Ø¹Ø¯Ø§Ø¯ Flask
app = Flask(__name__)
CORS(app)

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
valid_image_extensions = ['.png', '.jpg', '.jpeg']
valid_image_extensions_file_type = ['png', 'jpg', 'jpeg']
valid_doc_extensions = [
    # Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ…Ø³ØªÙ†Ø¯Ø§Øª Office
    '.txt', '.docx', '.docs', '.pdf', '.csv', '.xlsx', '.html', '.json',

    # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    '.sql', '.sqlite', '.db', '.bson', '.cql', '.neo4j',

    # Ù…Ù„ÙØ§Øª Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„ØªÙƒÙˆÙŠÙ†
    '.xml', '.yaml', '.yml', '.ini', '.conf', '.cfg',

    # Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©
    '.py', '.js', '.java', '.cpp', '.c', '.rb', '.php', '.ts', '.swift', '.go',
    '.cs', '.vb', '.scala', '.kt', '.rs', '.r', '.jl', '.pl', '.sh', '.bat', '.asm',
    '.lua', '.dart', '.erl', '.exs', '.ml', '.clj', '.fs', '.groovy', '.ps1', '.m',
    '.sas', '.sps', '.do', '.nb', '.tcl', '.ahk', '.applescript', '.vbs', '.tex',
    '.md', '.org', '.hs', '.adb', '.ads', '.for', '.f', '.f90',

    # Ù…Ù„ÙØ§Øª Ø§Ù„ØªØµÙ…ÙŠÙ…
    '.css', '.scss', '.sass', '.less', '.styl'
]

valid_doc_extensions_file_type = [
    # Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ…Ø³ØªÙ†Ø¯Ø§Øª Office
    'txt', 'docx', 'docs', 'pdf', 'csv', 'xlsx', 'html', 'json',

    # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    'sql', 'sqlite', 'db', 'bson', 'cql', 'neo4j',

    # Ù…Ù„ÙØ§Øª Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„ØªÙƒÙˆÙŠÙ†
    'xml', 'yaml', 'yml', 'ini', 'conf', 'cfg',

    # Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©
    'py', 'js', 'java', 'cpp', 'c', 'rb', 'php', 'ts', 'swift', 'go',
    'cs', 'vb', 'scala', 'kt', 'rs', 'r', 'jl', 'pl', 'sh', 'bat', 'asm',
    'lua', 'dart', 'erl', 'exs', 'ml', 'clj', 'fs', 'groovy', 'ps1', 'm',
    'sas', 'sps', 'do', 'nb', 'tcl', 'ahk', 'applescript', 'vbs', 'tex',
    'md', 'org', 'hs', 'adb', 'ads', 'for', 'f', 'f90',

    # Ù…Ù„ÙØ§Øª Ø§Ù„ØªØµÙ…ÙŠÙ…
    'css', 'scss', 'sass', 'less', 'styl'
]

@app.route('/ask', methods=['POST'])
def ask():
    data = request.json
    question = data.get('question')
    target_language = data.get('target_language')
    filebook = data.get('filebook')
    model_app = data.get('model_app')
    selectedModel = model_app.get('selectedModel')
    selectedCompany = model_app.get('selectedCompany')
    chat_type = data.get('chat_type')
    chat_id = data.get('chat_id')
    # Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø§ÙƒØ§Ø¯ÙŠÙ…ÙŠ
    field =  data.get('field')
    pages =  data.get('pages')
    target_audience =  data.get('target_audience')
    methodology = data.get('methodology')
    structure =  data.get('structure')
    citation_style =  data.get('citation_style')
    references_count =  data.get('references_count')
    start_year =  data.get('start_year')
    end_year =  data.get('end_year')
    include_lit_review =  data.get('include_lit_review')
    include_summary =  data.get('include_summary')
    academic_language_level =  data.get('academic_language_level')

     # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ© Ù„Ù„ØªÙ‚Ù„ÙŠØµ (Ù…Ø«Ù„Ø§Ù‹ 30%)
    reduction_percentage = 0.15

    # ØªÙ‚Ù„ÙŠØµ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø¨Ù€ 30%
    selectedModel["input_tokens"] = round(selectedModel["input_tokens"] * (1 - reduction_percentage))

# Ø§Ø³ØªØ±Ø¬Ø§Ø¹ max_tokens Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ‚Ù„ÙŠØµ
    max_tokens = selectedModel.get("input_tokens")
    # ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… max_tokens Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©

    file_type = data.get('file_type')
    file_path = data.get('file_path')
    file_name = data.get('file_name')

    max_tokens = int(max_tokens)
    word_limit = 870

    if not question:  # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„
        return jsonify({"error": "Please enter a message"}), 400

    relevant_text = ""
    question_type = ""

    if chat_type == "chat":
        relevant_text = ""
        question_type = ""

        # Ø­Ø³Ø§Ø¨ Ø­Ø§Ù„Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        total_tokens = calculate_tokens_for_memory(chat_id)  # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        memory_status = "Normal"  # Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        if total_tokens > max_tokens:  # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙÙˆÙ‚ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
            memory_status = "Over Full"
        elif total_tokens > max_tokens * 0.8:  # 80% Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
            memory_status = "Warning"
        elif total_tokens > max_tokens / 2:
            memory_status = "Normal - Memory is operating normally"
        else:
            memory_status = "Normal"


    elif chat_type == "translation":
        relevant_text = ""
        question_type = ""
        memory_status = ""

    elif chat_type == "academic_research_writing":
        relevant_text = ""
        question_type = ""
        memory_status = ""

    else:

        file_path__ = f"storage/books/{filebook}"  # Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù„Ù„Ù…Ù„Ù
        base_url = "https://hl-ai.kulshy.online"  # Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…ÙˆÙ‚Ø¹

        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„ÙˆØ¸ÙŠÙØ©
        book_content = fetch_and_read_file(file_path__, base_url)
        book_content = clean_text(book_content)
        question_type = analyze_question(question)
        relevant_text = retrieve_relevant_text(question, book_content, word_limit)
        memory_status = ""


    # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯Ø§Ù„Ø© generate_response Ù…Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
    response = {
        'answer': generate_response(relevant_text, question, question_type, selectedModel, selectedCompany, chat_id, chat_type, max_tokens, files=file_path ,file_type=file_type, file_name=file_name, target_language=target_language, field=field,pages=pages, methodology=methodology,target_audience=target_audience,citation_style=citation_style, structure=structure,include_summary=include_summary, academic_language_level=academic_language_level, include_lit_review=include_lit_review, start_year=start_year, references_count=references_count),
        'book_piece': relevant_text,
        'question': question,
        'memory_status': memory_status # Ø¥Ø¶Ø§ÙØ© Ø­Ø§Ù„Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    }

    return jsonify({"response": response})


def fetch_and_read_file(file_path__, base_url):
    """
    ÙŠØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù Ù…Ø­Ù„ÙŠÙ‹Ø§. Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·.
    ÙŠÙ‚Ø±Ø£ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù ÙˆÙŠØ¹ÙŠØ¯Ù‡.

    Args:
        file_path (str): Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù„Ù„Ù…Ù„Ù (Ù…Ø«Ù„ "storage/books/filename.txt").
        base_url (str): Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª.

    Returns:
        str: Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ø¥Ø°Ø§ Ù†Ø¬Ø­.
        dict: Ø±Ø³Ø§Ù„Ø© Ø®Ø·Ø£ Ø¥Ø°Ø§ Ø­Ø¯Ø«Øª Ù…Ø´ÙƒÙ„Ø©.
    """
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ù„ÙŠ Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ù„ÙØ§Øª
    storage_dir = os.path.join(os.path.dirname(file), '..', 'public', 'storage', 'books')
    os.makedirs(storage_dir, exist_ok=True)  # ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯

    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„
    local_file_path = os.path.join(storage_dir, os.path.basename(file_path__))

    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù Ù…Ø­Ù„ÙŠÙ‹Ø§
        if not os.path.exists(local_file_path):
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
            file_url = f"{base_url}/{file_path__}"
            response = requests.get(file_url, stream=True)
            response.raise_for_status()  # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø£ÙŠ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø·Ù„Ø¨

            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø­Ù„ÙŠÙ‹Ø§
            with open(local_file_path, 'wb') as file:
                for chunk in response.iter_content(chunk_size=8192):
                    file.write(chunk)

        # Ù‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        with open(local_file_path, 'r', encoding='utf-8') as file:
            content = file.read()

        return content

    except requests.exceptions.RequestException as e:
            return {"error": f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·: {e}"}
    except Exception as e:
            return {"error": f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}"}

@app.route('/clear_memory', methods=['POST'])
def clear_memory():
    chat_id = request.json.get("chat_id")
    if chat_id in conversation_history_dict:
        conversation_history_dict[chat_id] = []
        return jsonify({"message": "Memory cleared successfully."}), 200
    return jsonify({"error": "Chat ID not found."}), 404

file = None

@app.route('/upload-file', methods=['POST'])
def upload_file():
    if 'file' not in request.files:  # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù
        return jsonify({'error': 'No file part'}), 400

    file = request.files['file']

    if file.filename == '':  # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù‡Ù†Ø§Ùƒ Ù…Ù„ÙÙ‹Ø§
        return jsonify({'error': 'No selected file'}), 400

    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯
    _, ext = os.path.splitext(file.filename.lower())

    if not (ext in valid_image_extensions or ext in valid_doc_extensions):
        return jsonify({'error': 'Invalid file format, please upload a PNG, JPG, TXT, DOCX, or PDF file'}), 400

    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ "uploads"
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")  # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø²Ù…Ù†ÙŠ
    upload_folder = os.path.join("public", "uploads")
    os.makedirs(upload_folder, exist_ok=True)  # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
    new_filename = f"{file.filename}_{timestamp}{ext}"  # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ§Ø±ÙŠØ® Ø¥Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
    file_path = os.path.join(upload_folder, new_filename)
    file.save(file_path)

    if ext in valid_image_extensions:
        file_type = 'image'
    elif ext in valid_doc_extensions:
        file_type = 'document'
    else:
        file_type = 'unknown'  # Ø­Ø§Ù„Ø© Ù†Ø§Ø¯Ø±Ø© Ø¬Ø¯Ø§

    return jsonify({'message': f'{file_type.capitalize()} uploaded successfully', 'file_path': file_path, 'file_name':new_filename, 'file_type': ext[1:]}), 200


def generate_response(relevant_text, question, question_type, selectedModel, selectedCompany, chat_id, chat_type, max_tokens, files=None, file_type=None, file_name=None,target_language=None, field=None, pages=None, methodology=None,target_audience=None,citation_style=None, structure=None,include_summary=None, academic_language_level=None, include_lit_review=None, start_year=None, end_year=None, references_count=None):
    # prompt = f" Ø§Ù†Øª Ø®Ø¨ÙŠØ± Ø§ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ø§Ù‚Ø±Ø£ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØ£Ø¬Ø¨ ÙÙ‚Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„ÙƒØªØ§Ø¨ :\n\n{relevant_text}\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {question}"
    prompt = f"""
        Ø§Ù†Øª Ø®Ø¨ÙŠØ± Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØŒ Ø§Ù‚Ø±Ø£ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØ£Ø¬Ø¨ ÙÙ‚Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„ÙƒØªØ§Ø¨.\n"
        -  Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙˆØ§Ù†ÙŠÙ†ØŒ Ø£Ùˆ Ø¹Ù„Ø§Ù‚Ø§ØªØŒ Ø£Ùˆ ØªØ¹Ø§Ø±ÙŠÙ ÙˆØ±Ø¯Øª ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨ (ÙˆÙ„Ùˆ Ø¯ÙˆÙ† Ø£Ù…Ø«Ù„Ø©).\n"
        - Ø´Ø±Ø­ Ø£Ùˆ ØªÙØ³ÙŠØ±Ø§Øª ÙŠÙ…ÙƒÙ† Ù…Ù† Ø®Ù„Ø§Ù„Ù‡Ø§ Ø¨Ù†Ø§Ø¡ Ù…Ù†Ø·Ù‚ ÙˆØ§Ø¶Ø­ Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø¬ÙˆØ§Ø¨.\n\n"
        {relevant_text}\n\n"
        Ø§Ù„Ø³Ø¤Ø§Ù„: {question}"
    """

    if chat_type == "chatbook":
        if question_type == "boolean":
            prompt += "\n\nØ£Ø¬Ø¨ Ø¨Ù€ 'ØµØ­ÙŠØ­' Ø£Ùˆ 'Ø®Ø·Ø£' Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒØªØ§Ø¨ ÙˆØ§Ø°ÙƒØ± Ø³Ø¨Ø¨ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø§Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„ÙƒØªØ§Ø¨ ."
        elif question_type == "short_answer":
            prompt += "\n\nØ£Ø¬Ø¨ Ø¨Ø¥Ø¬Ø§Ø¨Ø© ØªØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨."
        elif question_type == "multiple_choice":
            prompt += "\n\n Ø§Ø®ØªØ± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© ÙˆØ§Ø°ÙƒØ± Ø³Ø¨Ø¨ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø§Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„ÙƒØªØ§Ø¨ ."
        elif question_type == "term":
            prompt += "\n\n Ø§Ù„Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµØ·Ù„Ø­ Ø§Ù„Ø§ØªÙŠ"
        elif question_type == "scientific_problem":
            prompt += "\n\n Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø£Ù„Ø© Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ø¹Ø·Ù‰."
        elif question_type == "justify":
            prompt += "\n\n Ø¹Ù„Ù‘Ù„ Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ù„Ø§Ø³ØªÙ†Ø§Ø¯ Ø¥Ù„Ù‰ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ø¹Ø·Ù‰."
        elif question_type == "ar_analysis":
            prompt += "\n\n Ø§Ø¹Ø±Ø¨ Ø§Ø°Ø§ Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ø¹Ø±Ø§Ø¨ ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© ÙÙŠÙƒ Ø¨Ø²ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ Ø¬Ø¯Ø§ ."
        elif question_type == "ai":
            prompt = f"\n\n{relevant_text}\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {question}\n\nØ£Ù†Øª Ø®Ø¨ÙŠØ± Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ. Ø§Ø³ØªØ®Ø¯Ù… Ù‚Ø¯Ø±Ø§ØªÙƒ ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø­Ø±ÙŠØ©ØŒ Ø¥Ø° Ù‚Ø¯ Ù„Ø§ ØªÙƒÙˆÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨."

    elif chat_type == "translation":

        prompt = f"""
            Ø£Ø±ÙŠØ¯ Ù…Ù†Ùƒ Ø£Ù† ØªØ¹Ù…Ù„ ÙƒÙ…ØªØ±Ø¬Ù… Ø§Ø­ØªØ±Ø§ÙÙŠ. Ø³Ø£Ø¹Ø·ÙŠÙƒ Ù„ØºØ© Ø§Ù„Ù‡Ø¯Ù ÙˆÙ†ØµÙ‹Ø§ØŒ ÙˆÙ…Ù‡Ù…ØªÙƒ Ø£Ù† ØªØªØ±Ø¬Ù… Ø§Ù„Ù†Øµ Ø¨Ø¯Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰ ÙˆØ§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ.

            Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©: {target_language}
            Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ØªØ±Ø¬Ù…ØªÙ‡:
            {question}
             ØªØ±Ø¬Ù… Ø§Ù„Ù†Øµ ÙÙ‚Ø· Ø¯ÙˆÙ† Ø´Ø±Ø­ Ø£Ùˆ ØªÙØ³ÙŠØ± Ø£Ùˆ ØªÙƒØ±Ø§Ø±.
            """
    elif chat_type == "academic_research_writing":

        def fetch_references(question, references_count=None, start_year=None, end_year=None):
            url = "https://api.semanticscholar.org/graph/v1/paper/search"
            params = {
                "query": question,
                "limit": references_count + 5,  # Ù„Ø¬Ù„Ø¨ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù‚Ù„ÙŠÙ„Ø§Ù‹
                "fields": "title,authors,year,venue,url"
            }

            response = requests.get(url, params=params)
            references = []

            if response.status_code == 200:
                data = response.json().get("data", [])

                # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ø³Ù†Ø© Ø¨Ø¯Ø§ÙŠØ© ÙˆÙ†Ù‡Ø§ÙŠØ© Ø¥Ø°Ø§ ØªÙ… ØªØ­Ø¯ÙŠØ¯Ù‡Ù…
                if start_year is not None and end_year is not None:
                    data = [p for p in data if start_year <= p.get("year", 0) <= end_year]
                elif start_year is not None:
                    data = [p for p in data if p.get("year", 0) >= start_year]
                elif end_year is not None:
                    data = [p for p in data if p.get("year", 0) <= end_year]

                # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ù„Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
                data = data[:references_count]

                for paper in data:
                    authors = ", ".join([a['name'] for a in paper.get("authors", [])]) if paper.get("authors") else "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
                    year = paper.get("year", "n.d.")
                    title = paper.get("title", "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†")
                    venue = paper.get("venue", "Ù…ÙƒØ§Ù† Ø§Ù„Ù†Ø´Ø± ØºÙŠØ± Ù…Ø­Ø¯Ø¯")
                    url = paper.get("url", "")
                    ref = f"{authors} ({year}). {title}. {venue}. Retrieved from {url}"
                    references.append(ref)
            else:
                references.append("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø¹ Ø¨Ø³Ø¨Ø¨ Ø¶Ø¹Ù Ø§Ù„Ø§Ù†ØªØ±Ù†Øª  .")

            return references

        # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ù…Ø¹ ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ø³Ù†Ø© ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        references_semanticscholar = fetch_references(
            question=question,
            references_count=references_count,
            start_year=start_year,
            end_year=end_year
        )

        def fetch_references_crossref(question, references_count=None, start_year=None, end_year=None):
            url = "https://api.crossref.org/works"
            filters = []
            if start_year:
                filters.append(f"from-pub-date:{start_year}")
            if end_year:
                filters.append(f"until-pub-date:{end_year}")
            params = {
                "query": question,
                "rows": references_count + 5,
                "filter": ",".join(filters) if filters else None,
                "sort": "relevance"
            }
            response = requests.get(url, params={k: v for k, v in params.items() if v})
            references = []

            if response.status_code == 200:
                items = response.json()["message"]["items"][:references_count]
                for item in items:
                    authors = ", ".join([f"{a.get('family', '')} {a.get('given', '')}".strip() for a in item.get("author", [])]) or "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
                    year = item.get("issued", {}).get("date-parts", [[None]])[0][0] or "n.d."
                    title = item.get("title", ["Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"])[0]
                    journal = item.get("container-title", ["Ù…ÙƒØ§Ù† Ø§Ù„Ù†Ø´Ø± ØºÙŠØ± Ù…Ø­Ø¯Ø¯"])[0]
                    url = item.get("URL", "")
                    references.append(f"{authors} ({year}). {title}. {journal}. Retrieved from {url}")
            else:
                references.append("âš ï¸")
            return references

# Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ù…Ø¹ ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ø³Ù†Ø© ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        references_crossref = fetch_references_crossref(
            question=question,
            references_count=references_count,
            start_year=start_year,
            end_year=end_year
        )

        references = references_semanticscholar + references_crossref
        # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ù„Ù„Ø¹Ø±Ø¶ Ø¶Ù…Ù† Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª
        formatted_references = "\n".join(f"- {ref}" for ref in references)

        prompt = f"""
            Ø£Ø±ÙŠØ¯Ùƒ Ø£Ù† ØªÙƒØªØ¨ Ø¨Ø­Ø«Ù‹Ø§ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠÙ‹Ø§ Ø´Ø§Ù…Ù„Ø§Ù‹ ÙˆÙ…ÙØµÙ„Ø§Ù‹ØŒ ÙˆÙƒØ£Ù†Ùƒ Ø·Ø§Ù„Ø¨ Ø¬Ø§Ù…Ø¹ÙŠ Ù…Ø¬ØªÙ‡Ø¯ Ø£Ùˆ Ø¨Ø§Ø­Ø« Ø­Ù‚ÙŠÙ‚ÙŠ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©.

            Ø³ØªÙ‚ÙˆÙ… Ø¨ÙƒØªØ§Ø¨Ø© Ø¨Ø­Ø« Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ø´Ø§Ù…Ù„ ÙˆÙ…ÙØµÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:

            - ğŸ“š Ø§Ù„ØªØ®ØµØµ: {field}
            - ğŸ“ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø¨Ø­Ø«: {question}
            - ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: {pages}
            - ğŸ§‘â€ğŸ“ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©: {target_audience}
            - ğŸ” Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„Ù…ØªØ¨Ø¹Ø©: {methodology}
            - ğŸ§± Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {structure}
            - ğŸ“Œ Ù†Ù…Ø· Ø§Ù„ØªÙˆØ«ÙŠÙ‚: {citation_style}
            - ğŸ”¢ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: {references_count}
           - ğŸ“… Ø­Ø¯Ø§Ø«Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹: Ù…Ù† {start_year} Ø¥Ù„Ù‰ {end_year}
            - ğŸ—ƒï¸ Ù‡Ù„ ØªØªØ¶Ù…Ù† Ù…Ø±Ø§Ø¬Ø¹Ø© Ø£Ø¯Ø¨ÙŠØ©ØŸ {'Ù†Ø¹Ù…' if include_lit_review else 'Ù„Ø§'}
            - ğŸ“‘ Ù‡Ù„ ÙŠØªØ¶Ù…Ù† Ù…Ù„Ø®ØµÙ‹Ø§ ØªÙ†ÙÙŠØ°ÙŠÙ‹Ø§ØŸ {'Ù†Ø¹Ù…' if include_summary else 'Ù„Ø§'}
            - ğŸŒ Ù„ØºØ© Ø§Ù„Ø¨Ø­Ø«: {academic_language_level}

            Ø§ÙƒØªØ¨ Ø§Ù„Ø¨Ø­Ø« ÙƒØ§Ù…Ù„Ø§Ù‹ Ø«Ù… Ø§Ø®ØªØªÙ…Ù‡ Ø¨Ù‚Ø³Ù… Ø¨Ø¹Ù†ÙˆØ§Ù† "Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹"ØŒ ÙŠØªØ¶Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ¨Ù†Ù…Ø· {citation_style}:
            {formatted_references}

            Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:
            - Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ù„ÙˆØ¨Ù‹Ø§ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠÙ‹Ø§ ÙˆØ§Ù‚Ø¹ÙŠÙ‹Ø§ØŒ ÙŠØ´Ø¨Ù‡ Ù…Ø§ ÙŠÙƒØªØ¨Ù‡ Ø·Ø§Ù„Ø¨ Ø¬Ø§Ù…Ø¹ÙŠ Ø£Ùˆ Ø¨Ø§Ø­Ø« ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©.
            - Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ù„ØºØ© Ù…Ø«Ø§Ù„ÙŠØ© Ø£Ùˆ Ù…ØµÙ‚ÙˆÙ„Ø© Ø¬Ø¯Ù‹Ø§ØŒ Ø¨Ù„ Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø·Ø¨ÙŠØ¹ÙŠÙ‹Ø§ØŒ Ø¯ÙˆÙ† ØªØµÙ†Ø¹ Ø£Ùˆ ØªÙƒÙ„Ù‘Ù.
            - Ù„Ø§ ØªÙƒØ±Ø± Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ.
            - Ù„Ø§ Ø¨Ø£Ø³ Ø¨Ø¨Ø¹Ø¶ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ø£Ùˆ Ø§Ù„ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ.
            - Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¨Ø´Ø±ÙŠØ© Ù…ÙÙ‡ÙˆÙ…Ø© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¹Ù„Ù…ÙŠØ©.
            - Ø§Ù„ØªØ²Ù… Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ ÙˆØ£Ù†Ù‡Ù Ø§Ù„Ø¨Ø­Ø« Ø¨Ù‚Ø³Ù… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ ÙˆÙÙ‚ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø¯Ø¯.

            Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø¢Ù† ÙÙŠ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¨Ø­Ø« Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø§ Ø³Ø¨Ù‚:
            """


    else:
        prompt = question


    response = ""
    """
    Ø¯Ø§Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù†ØµØ© OpenAI Ø£Ùˆ Gemini Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
    """
    model_key = selectedModel.get("model")  # Ù…Ø«Ù„ "gpt-4o"
    api_key = selectedModel.get("api_key")  # Ø§Ù„Ù…ÙØªØ§Ø­ API
    model_name = selectedModel.get("name_model")  # Ù…Ø«Ù„ "GPT 4o"
    base_url = selectedModel.get("base_url")  # Ø¹Ù†ÙˆØ§Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ Ø§Ù„API
    input_tokens = selectedModel.get("input_tokens")
    output_tokens = selectedModel.get("output_tokens")
    response = ""

    if selectedCompany.get('name') == "Open Ai":
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙØªØ§Ø­ API Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
        openai.api_key = api_key
        openai.api_base = base_url
        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ø§Ù„ØµÙˆØ±Ø© (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©)
        # image_url = upload_to_openai("",  api_key=api_key)

        try:
                # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØµÙˆØ±Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© Ø£Ùˆ Ù„Ø§ØŒ ÙŠØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙÙ‚Ø·
            if files:
                # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØµÙˆØ±Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø©
                updated_history, memory_error = update_conversation_history(chat_id, prompt, max_tokens, "openai", files, file_type, file_name)
            else:
                # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø§Ù„ØµÙˆØ±Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ù†Ø±Ø³Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø©
                updated_history, memory_error = update_conversation_history(chat_id, prompt, max_tokens, "openai")

            # ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù…ØªÙ„Ø¦Ø©
            if memory_error:
                return memory_error  # Ø¥Ø±Ø¬Ø§Ø¹ Ø±Ø³Ø§Ù„Ø© ØªÙˆØ¶Ø­ Ø£Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù…ØªÙ„Ø¦Ø©

            if chat_type == "chat":
               # Ø­Ø³Ø§Ø¨ Ø­Ø§Ù„Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                total_tokens = calculate_tokens_for_memory(chat_id)  # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
                total_tokens = int(total_tokens)

                if total_tokens > max_tokens:  # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙÙˆÙ‚ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰

                    response = "Memory is full. Please delete the memory or start a new chat." # Ø­Ø§Ù„Ø© "over Full"
                    return response # Ø­Ø§Ù„Ø© "over Full"
                else:
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© API Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
                    stream = openai.ChatCompletion.create(
                        temperature=0.2,
                        top_p=1.0,
                        model=model_key,
                        messages=updated_history,
                        max_tokens=output_tokens,
                        stream=True
                    )

            elif chat_type == "academic_research_writing":
               # Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© API Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø¯ÙˆÙ† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
                stream = openai.ChatCompletion.create(
                    model=model_key,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=8096,
                    stream=True
                )

            else:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© API Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø¯ÙˆÙ† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
                stream = openai.ChatCompletion.create(
                    model=model_key,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=output_tokens,
                    stream=True
                )

            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªÙ„Ù…Ø© Ù…Ù† Ø§Ù„Ø¨Ø« (stream)
            for chunk in stream:
                if chunk.get('choices') and chunk['choices'][0].get('delta') and chunk['choices'][0]['delta'].get('content'):
                    response += chunk['choices'][0]['delta']['content']  # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙ„Ù‚Ø§Ø©

            if chat_type == "chat":
                # Ø¥Ø¶Ø§ÙØ© Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
                conversation_history_dict[chat_id].append({"role": "assistant", "content": response})

        except openai.error.OpenAIError as e:
            response = f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨: {str(e)}"
        except Exception as e:
            response = f"Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}"

        return response

    elif selectedCompany.get('name') == "Gemini":
        # ØªÙ‡ÙŠØ¦Ø© API Ù…Ø¹ Ø§Ù„Ù…ÙØªØ§Ø­
        genai.configure(api_key=api_key)
        if files != None and file_type == 'png' or file_type == 'jpg' or file_type == 'jpeg':
            files = upload_to_gemini(files, mime_type="image/jpeg"),

        # ØªÙƒÙˆÙŠÙ† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯
        generation_config = {
            "temperature": 0.2,
            "top_p": 1.0,
            "top_k": 40,
            "max_output_tokens": output_tokens,
            "response_mime_type": "text/plain",
        }

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠ
        model = genai.GenerativeModel(
            model_name=model_key,
            generation_config=generation_config
        )

        if chat_type == "chat":
            # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù…Ù†ØµØ© Gemini
            updated_history, memory_error = update_conversation_history(chat_id, prompt, max_tokens, "gemini", files, file_type, file_name)

            # ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù…ØªÙ„Ø¦Ø©
            if memory_error:
                return memory_error  # Ø¥Ø±Ø¬Ø§Ø¹ Ø±Ø³Ø§Ù„Ø© ØªÙˆØ¶Ø­ Ø£Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù…ØªÙ„Ø¦Ø©
            # Ø­Ø³Ø§Ø¨ Ø­Ø§Ù„Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            total_tokens = calculate_tokens_for_memory(chat_id)  # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
            total_tokens = int(total_tokens)

            if total_tokens > max_tokens:  # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙÙˆÙ‚ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
                response = "Memory is full. Please delete the memory or start a new chat."
                return response
            else:
                # Ø¨Ø¯Ø¡ Ø¬Ù„Ø³Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø¯Ø«
                chat_session = model.start_chat(history=updated_history["contents"])
        else:

            chat_session = model.start_chat(history=[{
                "role": "user",
                "parts": [prompt],
            }])

        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        try:
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
            response_obj = chat_session.send_message(prompt)

# Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ØµØŒ Ù†Ø¶ÙŠÙÙ‡
            if hasattr(response_obj, 'text'):
                response = response_obj.text  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ø³ØªÙ„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬

            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø®Ø§ØµÙŠØ© is_final (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©)
            if hasattr(response_obj, 'is_final') and response_obj.is_final:
                return None

            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ù† Ù†ÙˆØ¹ 'chat'ØŒ Ù†Ø­Ø¯Ø« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨Ø§Ù„Ø±Ø¯
            if chat_type == "chat":
                updated_history, memory_error = update_conversation_history(chat_id, response, max_tokens, "gemini")
                if memory_error:
                    return memory_error  # Ø¥Ø±Ø¬Ø§Ø¹ Ø±Ø³Ø§Ù„Ø© ØªÙˆØ¶Ø­ Ø£Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù…ØªÙ„Ø¦Ø©

        except Exception as e:
            response = f"Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}"

        return response
    else:

        url = base_url
        api_key = api_key

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        if chat_type == "chat":
            # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø§Ù„ØµÙˆØ±Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ù†Ø±Ø³Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø©
            updated_history, memory_error = update_conversation_history(chat_id, prompt, max_tokens, "openai")
            # ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù…ØªÙ„Ø¦Ø©
            if memory_error:
                return memory_error  # Ø¥Ø±Ø¬Ø§Ø¹ Ø±Ø³Ø§Ù„Ø© ØªÙˆØ¶Ø­ Ø£Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù…ØªÙ„Ø¦Ø©
        else:
            updated_history = [{"role": "user", "content": prompt}]

        data = {
            "model": model_key,
            "messages": updated_history,
            "max_tokens": output_tokens,
            "temperature": 0.2,
            "top_p": 1.0,
            "top_k": 100,
            "repetition_penalty": 1,
            "stop": ["<|eot_id|>", "<|eom_id|>"],
            "stream": True
        }

        response = requests.post(url, headers=headers, data=json.dumps(data), stream=True)

        full_response = ""
        for chunk in response.iter_lines():
            if chunk:
                line = chunk.decode('utf-8')
                if line.startswith("data: "):
                    line = line[6:]
                try:
                    chunk_data = json.loads(line)
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† "choices" ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù†Ø§ØµØ± Ù‚Ø¨Ù„ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„ÙŠÙ‡Ø§
                    if "choices" in chunk_data and len(chunk_data["choices"]) > 0 and "delta" in chunk_data["choices"][0]:
                        content = chunk_data["choices"][0]["delta"].get("content", "")
                        full_response += content
                except json.JSONDecodeError as e:
                    er = f"Error decoding JSON: {e}"

        if chat_type == "chat":
            # Ø¥Ø¶Ø§ÙØ© Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            conversation_history_dict[chat_id].append({"role": "assistant", "content": full_response})

        return full_response

# def clean_text(text):
#     # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø©
#     text = text.lower()
#     # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙÙˆØ§ØµÙ„ Ø¹Ø´Ø±ÙŠØ©
#     text = re.sub(r'(?<=\d),(?=\d)', 'ØŒ', text)  # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„ÙÙˆØ§ØµÙ„ Ø§Ù„Ø¹Ø´Ø±ÙŠØ© Ø¨ÙÙˆØ§ØµÙ„ Ø¹Ø±Ø¨ÙŠØ©
#     # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ Ø±Ù…ÙˆØ² ØºÙŠØ± Ù…Ø±ØºÙˆØ¨ Ø¨Ù‡Ø§ Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
#     text = re.sub(r'[^\w\s\.,ØŒ\ØŸ\!]', '', text)
#     return text.strip()

def clean_text(text):
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø©
    text = text.lower()

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ (Ø§Ù„Ø­Ø±ÙƒØ§Øª Ù…Ø«Ù„ Ù‹ Ù ÙÙ‘ ÙŒ Ù)
    text = re.sub(r'[\u064B-\u0652]', '', text)

    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„ÙØ§ØµÙ„Ø© Ø§Ù„Ø¹Ø´Ø±ÙŠØ© Ø¨ÙØ§ØµÙ„Ø© Ø¹Ø±Ø¨ÙŠØ© (3,5 â†’ 3ØŒ5)
    text = re.sub(r'(?<=\d),(?=\d)', 'ØŒ', text)

    # Ø¥Ø²Ø§Ù„Ø© Ø±Ù…ÙˆØ² ØºÙŠØ± Ù…Ø±ØºÙˆØ¨Ø© (Ù…Ø«Ù„ ï´¾ØŒ ï·ºØŒ Û)
    text = re.sub(r'[^\w\s\.,ØŒ\ØŸ\!]', '', text)

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© ÙˆØ§Ù„Ù€ Unicode Ø§Ù„Ø²Ø®Ø±ÙÙŠ
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # ÙˆØ¬ÙˆÙ‡ ØªØ¹Ø¨ÙŠØ±ÙŠØ©
        u"\U0001F300-\U0001F5FF"  # Ø±Ù…ÙˆØ² Ù…ØªÙ†ÙˆØ¹Ø©
        u"\U0001F680-\U0001F6FF"  # Ù…Ø±ÙƒØ¨Ø§Øª ÙˆØ·Ø§Ø¦Ø±Ø§Øª

u"\U0001F1E0-\U0001F1FF"  # Ø£Ø¹Ù„Ø§Ù… Ø¯ÙˆÙ„
        u"\U00002500-\U00002BEF"  # Ø£Ø´ÙƒØ§Ù„ ÙˆØ±Ù…ÙˆØ²
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\u200d"
        u"\u2640-\u2642"
        u"\u2600-\u2B55"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"  # ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        u"\u3030"
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def analyze_question(question):
    question = clean_text(question)
    if re.search(r'\b(Ù‡Ù„|Ø£Ù„ÙŠØ³|Ù‡Ù„ ØªØ¹ØªØ¨Ø±|Ù‡Ù„ ÙŠÙ…ÙƒÙ†|ØµØ­ÙŠØ­ Ø£Ù… Ø®Ø·Ø£|Ù‡Ù„ Ù‡Ø°Ø§|boolean)\b', question):
        return "boolean"
    elif re.search(r'\b(Ù…Ø§ Ù‡Ùˆ|Ø¹Ø±Ù|Ø§Ø°ÙƒØ±|Ø§Ø­Ø³Ø¨|Ø­Ù„|short_answer)\b', question):
        return "short_answer"
    elif re.search(r'\b(Ø§Ø®ØªØ±|Ø§Ø®ØªØ± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©|Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯|multiple_choice)\b', question):
        return "multiple_choice"
    elif re.search(r'\b(Ø¹Ù„Ù„)\b', question):
        return "justify"
    elif re.search(r'\b(Ø§Ø¹Ø±Ø¨|Ø¥Ø¹Ø±Ø§Ø¨|Ø§Ø¹Ø±Ø§Ø¨)\b', question):
        return "ar_analysis"
    elif re.search(r'\b(Ø­Ù„|Ø§Ø­Ø³Ø¨|ÙƒÙŠÙ|ØªÙØ³ÙŠØ±|ÙØ³Ø±|scientific_problem)\b', question):
        return "scientific_problem"
    elif re.search(r'\b(Ø­Ù„|Ø§Ù„Ù…ØµØ·Ù„Ø­ Ø§Ù„Ø§ØªÙŠ|Ø§Ù„Ù…ØµØ·Ù„Ø­ Ø§Ù„Ø§ØªÙŠ|Ø§Ù„Ù…ØµØ·Ù„Ø­|Ù…ØµØ·Ù„Ø­|term)\b', question):
        return "term"
    elif re.search(r'\b(ai|Ai)\b', question):
        return "ai"
    else:
        return "long_answer"










# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
model = SentenceTransformer("all-MiniLM-L6-v2")

# ğŸ”¹ ÙˆØ¸ÙŠÙØ© ØªÙ†Ù‚ÙŠØ© Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙˆØ¯Ù…Ø¬Ù‡Ø§ Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­
def remove_duplicates_and_merge(tfidf_text, semantic_text):
    tfidf_sentences = [s.strip() for s in re.split(r'(?<=[\.ØŒ\ØŸ\!])\s+', tfidf_text) if s.strip()]
    semantic_sentences = [s.strip() for s in re.split(r'(?<=[\.ØŒ\ØŸ\!])\s+', semantic_text) if s.strip()]

    tfidf_set = set(tfidf_sentences)
    filtered_semantic_sentences = [s for s in semantic_sentences if s not in tfidf_set]

    combined_sentences = tfidf_sentences + filtered_semantic_sentences
    final_text = '\n'.join(f"â€¢ {s}" for s in combined_sentences)
    return final_text

# ğŸ”¹ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def retrieve_relevant_text_with_ai(question, book_content, word_limit, min_similarity=0.05, max_sentences_ai=20):
    if len(book_content.split()) <= word_limit:
        print("[INFO] Ù†Øµ Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ù‚Ù„ Ø£Ùˆ ÙŠØ³Ø§ÙˆÙŠ Ø­Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ØŒ ÙŠØ±Ø¬Ø¹ Ø§Ù„Ù†Øµ ÙƒØ§Ù…Ù„.")
        return book_content

    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¬Ù…Ù„
    sentences = re.split(r'(?<=[\.ØŒ\ØŸ\!])\s+', book_content)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]
    print(f"[INFO] Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø³ÙŠÙ…: {len(sentences)}")

    if not sentences:
        return "Ø§Ù„Ù†Øµ ÙØ§Ø±Øº Ø£Ùˆ Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¬Ù…Ù„ Ù…ÙÙ‡ÙˆÙ…Ø©."

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF
    vectorizer = TfidfVectorizer(ngram_range=(1, 3))
    vectors = vectorizer.fit_transform([question] + sentences)
    cosine_similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()
    sorted_indices = cosine_similarities.argsort()[::-1]
    print(f"[INFO] Ø£Ø¹Ù„Ù‰ Ø¯Ø±Ø¬Ø© ØªØ´Ø§Ø¨Ù‡: {cosine_similarities[sorted_indices[0]]:.4f}")

    selected_sentences = []
    total_words = 0

    for index in sorted_indices:
        sentence = sentences[index]
        similarity_score = cosine_similarities[index]

        if similarity_score < min_similarity:
            print(f"[DEBUG] ØªØ´Ø§Ø¨Ù‡ Ù…Ù†Ø®ÙØ¶ Ø¬Ø¯Ø§Ù‹ {similarity_score:.4f}ØŒ Ø§Ù„ØªÙˆÙ‚Ù.")
            break

        sentence_word_count = len(sentence.split())
        if total_words + sentence_word_count > word_limit:
            print("[DEBUG] ØªØ¬Ø§ÙˆØ² Ø­Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨.")
            break

        selected_sentences.append(sentence)
        total_words += sentence_word_count

    if total_words < word_limit:
        for index in sorted_indices[len(selected_sentences):]:
            sentence = sentences[index]
            sentence_word_count = len(sentence.split())
            if total_words + sentence_word_count > word_limit:
                break
            selected_sentences.append(sentence)
            total_words += sentence_word_count

    tfidf_result = ' '.join(selected_sentences) if selected_sentences else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¬Ù…Ù„ Ù…Ø´Ø§Ø¨Ù‡Ø© ÙƒØ§ÙÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„."
    print(f"[INFO] Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© Ù…Ù† TF-IDF: {total_words}")

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
    top_sentences_for_ai = [sentences[i] for i in sorted_indices[:max_sentences_ai]]
    if len(top_sentences_for_ai) == 0:
        semantic_result = "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¬Ù…Ù„ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ."
    else:
        sentence_embeddings = model.encode(top_sentences_for_ai, convert_to_tensor=True, batch_size=16)
        question_embedding = model.encode(question, convert_to_tensor=True)

        semantic_similarities = util.cos_sim(question_embedding, sentence_embeddings)[0]
        sorted_indices_sem = torch.topk(semantic_similarities, k=len(top_sentences_for_ai)).indices.cpu().numpy()

        selected_semantic = []
        semantic_word_count = 0

        for idx in sorted_indices_sem:
            sentence = top_sentences_for_ai[idx]
            wc = len(sentence.split())
            if semantic_word_count + wc > word_limit:
                break
            selected_semantic.append(sentence)
            semantic_word_count += wc

        semantic_result = ' '.join(selected_semantic)
        print(f"[INFO] Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {semantic_word_count}")

    # ğŸ”¹ ØªÙ†Ù‚ÙŠØ© Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØ¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨ÙˆØ¶ÙˆØ­
    clean_combined = remove_duplicates_and_merge(tfidf_result, semantic_result)

    full_combined_result = f"""
ğŸ”¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† TF-IDF:
{tfidf_result}

----------------------------------------

ğŸ”¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:
{semantic_result}

----------------------------------------

ğŸ”¹ Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ù‚ÙŠØ©):
{clean_combined}
"""
    return full_combined_result.strip()









def upload_to_gemini(path, mime_type=None):
  """Uploads the given file to Gemini.

  See https://ai.google.dev/gemini-api/docs/prompting_with_media
  """
  file = genai.upload_file(path, mime_type=mime_type)
  return file

def upload_to_openai(path, api_key=""):
    """ÙŠØ±ØªÙØ¹ Ø¨Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ OpenAI Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… API Ø§Ù„Ù…Ù†Ø§Ø³Ø¨"""
    openai.api_key = api_key

    if not os.path.exists(path):
        return None

    with open(path, "rb") as file:
        try:
            # Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ OpenAI
            response = openai.Image.create(file=file, purpose='answers')

            if 'data' in response and len(response['data']) > 0:
                file_url = response['data'][0]['url']
                return file_url
            else:
                return None

        except openai.error.APIError as e:
            return None
        except Exception as e:
            return None

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ: ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø«Ù„ "gpt-4")
encoding = tiktoken.get_encoding("cl100k_base")  # ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©

def calculate_tokens(messages):
    total_tokens = 0

    for msg in messages:
        # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ 'text' Ø£Ùˆ 'content' Ø£Ùˆ 'parts' ÙÙŠ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
        if 'text' in msg:
            total_tokens += len(encoding.encode(msg['text']))
        elif 'content' in msg:
            total_tokens += len(encoding.encode(msg['content']))
        elif 'parts' in msg:
            for part in msg['parts']:
                if isinstance(part, dict) and 'text' in part:
                    total_tokens += len(encoding.encode(part['text']))
                elif isinstance(part, str):
                    # Ø§ÙØªØ±Ø¶ Ø£Ù† ÙƒÙ„ Ù…Ù„Ù Ù„Ù‡ Ù†Ø³Ø¨Ø© Ù…Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø§ØªØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø­Ø³Ø¨ Ø§Ø­ØªÙŠØ§Ø¬Ùƒ
                    total_tokens += 100  # Ø£Ùˆ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ù†Ø·Ù‚ Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù Ø£Ùˆ Ù†ÙˆØ¹Ù‡

    return total_tokens

conversation_history_dict = {}

def update_conversation_history(chat_id, user_message, max_tokens, platform, files=None, file_type=None, file_name=None):
    if chat_id not in conversation_history_dict:
        conversation_history_dict[chat_id] = []

    # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù„Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    new_message_tokens = len(user_message.split()) * 1.33
    required_tokens = new_message_tokens * 2  # Ù…Ø¶Ø§Ø¹ÙØ© Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨

# Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„ÙŠ Ù„Ù„ØªÙˆÙƒÙŠÙ†Ø§Øª Ù‚Ø¨Ù„ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    total_tokens = calculate_tokens(conversation_history_dict[chat_id])
    # Ø­Ø°Ù Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø­ØªÙ‰ ÙŠØªÙ… ØªØ­Ø±ÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    while (total_tokens + new_message_tokens) > max_tokens:
        if len(conversation_history_dict[chat_id]) >= 2:
            # Ø­Ø°Ù Ø±Ø³Ø§Ù„ØªÙŠÙ†
            conversation_history_dict[chat_id].pop(0)  # Ø­Ø°Ù Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
            conversation_history_dict[chat_id].pop(0)  # Ø­Ø°Ù Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©
        elif conversation_history_dict[chat_id]:
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ Ø±Ø³Ø§Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·ØŒ Ø§Ø­Ø°ÙÙ‡Ø§
            conversation_history_dict[chat_id].pop(0)

        # ØªØ­Ø¯ÙŠØ« Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
        total_tokens = calculate_tokens(conversation_history_dict[chat_id])


    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    if platform == "openai":
        if files:
            if file_type in valid_image_extensions_file_type:
                conversation_history_dict[chat_id].append({"role": "user", "content": "No found image"+ user_message})
            elif file_type in valid_doc_extensions_file_type:
                # Ù†ÙØ³ Ø§Ù„Ø´ÙŠØ¡ Ù‡Ù†Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… "in"
                content_file = process_file(files)
                user_message += "Name File: "+file_name+" content File:" + content_file +" "+ user_message
                conversation_history_dict[chat_id].append({"role": "user", "content": user_message})

            else:
                conversation_history_dict[chat_id].append({"role": "user", "content": "No content found in file"+ user_message})
        else:
            conversation_history_dict[chat_id].append({"role": "user", "content": user_message})
    elif platform == "gemini":
        if files:
            if file_type in valid_image_extensions_file_type:
                conversation_history_dict[chat_id].append({
                    "role": "user",
                    "parts": [files[0], {"text": user_message}]
                })
            elif file_type in valid_doc_extensions_file_type:  # Ù†ÙØ³ Ø§Ù„Ø´ÙŠØ¡ Ù‡Ù†Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… "in"
                content_file = process_file(files)
                user_message += "Name File: "+file_name+" content File: " + content_file + user_message
                conversation_history_dict[chat_id].append({
                    "role": "user",
                    "parts": [{"text": user_message}]
                })
            else:
                conversation_history_dict[chat_id].append({
                    "role": "user",
                    "parts": [{"text":"No content found in file"+ user_message}]
                })
        else:
            conversation_history_dict[chat_id].append({
                "role": "user",
                "parts": [{"text": user_message}]
            })

    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    total_tokens = calculate_tokens(conversation_history_dict[chat_id])

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ù„Ù„Ù…Ù†ØµØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
    if platform == "openai":
        return conversation_history_dict[chat_id], None
    elif platform == "gemini":
        gemini_history = []
        for msg in conversation_history_dict[chat_id]:
            if 'content' in msg:
                gemini_history.append({
                    "role": msg['role'],
                    "parts": [{"text": msg["content"]}]
                })
            elif 'parts' in msg:
                gemini_history.append({
                    "role": msg['role'],
                    "parts": msg["parts"]
                })
        return {"contents": gemini_history}, None
    else:
        return None, "Ù…Ù†ØµØ© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©."

# Ø¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø§Øª Ù„Ù„Ø°Ø§ÙƒØ±Ø©
def calculate_tokens_for_memory(chat_id):
    total_tokens = 0
    if chat_id in conversation_history_dict:
        total_tokens = calculate_tokens(conversation_history_dict[chat_id])  # Ø£Ùˆ Ø£ÙŠ Ù…Ù†ØµØ© Ø£Ø®Ø±Ù‰
    return total_tokens


def process_file(file_path):
    """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹Ù‡"""
    _, ext = os.path.splitext(file_path.lower())

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯
    if ext == '.txt':
        return extract_text_from_txt(file_path)
    elif ext in ['.docx', '.docs']:
        return extract_text_from_docx(file_path)
    elif ext == '.pdf':
        return extract_text_from_pdf(file_path)
    elif ext == '.csv':
        return extract_text_from_csv(file_path)
    elif ext == '.xlsx':
        return extract_text_from_excel(file_path)
    elif ext == '.html':
        return extract_text_from_html(file_path)
    elif ext == '.json':
        return extract_text_from_json(file_path)
    elif ext in ['.sql', '.sqlite', '.db', '.bson', '.cql', '.neo4j']:
        return extract_text_from_database(file_path)
    elif ext in ['.xml', '.yaml', '.yml', '.ini', '.conf', '.cfg']:
        return extract_text_from_config(file_path)
    elif ext in [
        '.py', '.js', '.java', '.cpp', '.c', '.rb', '.php', '.ts', '.swift', '.go',
        '.cs', '.vb', '.scala', '.kt', '.rs', '.r', '.jl', '.pl', '.sh', '.bat', '.asm',
        '.lua', '.dart', '.erl', '.exs', '.ml', '.clj', '.fs', '.groovy', '.ps1', '.m',
        '.sas', '.sps', '.do', '.nb', '.tcl', '.ahk', '.applescript', '.vbs', '.tex',
        '.md', '.org', '.hs', '.adb', '.ads', '.for', '.f', '.f90', '.css', '.scss', '.sass',
        '.less', '.styl'
    ]:
        return extract_text_from_code(file_path)
    else:
        return f"Unsupported file type: {ext}"

def extract_text_from_txt(file_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù Ù†ØµÙŠ (.txt)"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
        return text
    except Exception as e:
        return f"Error reading .txt file: {str(e)}"

def extract_text_from_docx(file_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù Word (.docx)"""
    try:
        doc = Document(file_path)
        text = '\n'.join([para.text for para in doc.paragraphs])
        return text
    except Exception as e:
        return f"Error reading .docx file: {str(e)}"

def extract_text_from_pdf(file_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù PDF"""
    try:
        text = ""
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            for page in reader.pages:
                text += page.extract_text() + '\n'
        return text
    except Exception as e:
        return f"Error reading .pdf file: {str(e)}"

def extract_text_from_csv(file_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù CSV"""
    try:
        text = ""
        with open(file_path, 'r', encoding='utf-8') as file:
            reader = csv.reader(file)
            for row in reader:
                text += ', '.join(row) + '\n'
        return text
    except Exception as e:
        return f"Error reading .csv file: {str(e)}"

def extract_text_from_excel(file_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù Excel (.xlsx)"""
    try:
        df = pd.read_excel(file_path)
        text = df.to_string(index=False)
        return text
    except Exception as e:
        return f"Error reading .xlsx file: {str(e)}"

def extract_text_from_html(file_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù HTML"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            soup = BeautifulSoup(file, 'html.parser')
            text = soup.get_text(separator='\n')
        return text
    except Exception as e:
        return f"Error reading .html file: {str(e)}"

def extract_text_from_json(file_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù JSON"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
        text = json.dumps(data, indent=4, ensure_ascii=False)
        return text
    except Exception as e:
        return f"Error reading .json file: {str(e)}"

def extract_text_from_database(file_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ SQL"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
        return text
    except Exception as e:
        return f"Error reading database file: {str(e)}"

def extract_text_from_config(file_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„ÙØ§Øª Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª (XML, YAML, INI)"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
        return text
    except Exception as e:
        return f"Error reading config file: {str(e)}"

def extract_text_from_code(file_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
        return text
    except Exception as e:
        return f"Error reading code file: {str(e)}"


# public_url = ngrok.connect(5002)
# print("ğŸ”— Ø±Ø§Ø¨Ø· Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ API:", public_url)

if name == 'main':
    app.run(host='0.0.0.0',port=5002)
